---
title: "Unsupervised Model"
author: "Your Name"
date: "2024-2-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,fig.align="center",fig.width=7,fig.height=7)

pacman::p_load(
       car
      , data.table
      , dtwclust
      , dplyr
      , fpc
      , ggplot2
      , ggExtra
      , keras
      , reshape2
      , corrplot
      , RColorBrewer
      , lubridate
      , M3C
      , mclust
      , NbClust
      , AmesHousing
      , vegan
      , RBM
      , Rtsne
      , umap
      )

```

```{css,echo=FALSE}
.btn {
    border-width: 0 0px 0px 0px;
    font-weight: normal;
    text-transform: ;
}

.btn-default {
    color: #2ecc71;
    background-color: #ffffff;
    border-color: #ffffff;
}
```

```{r,echo=FALSE}
# Global parameter
show_code <- TRUE
```

# Class Workbook {.tabset .tabset-fade .tabset-pills}

## In class activity
### Fraud Detection

The data is synthetic datasets generated by the PaySim mobile money simulator
https://www.kaggle.com/datasets/ealaxi/paysim1


```{r,echo=show_code}
#Fd<-fread("PS_20174392719_1491204439457_log.csv")
#saveRDS(Fd,file="fraud.rds")
# Fd=readRDS(file="fraud.rds"
# write.csv(Fd[sample(1:3000000,1000,FALSE),], file=gzfile("fraud.csv.gz"))
require(readr)
Fd<-read_csv("fraud.csv.gz")
```

PaySim simulates mobile money transactions based on a sample of actual transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, the mobile financial service provider, which is currently running in more than 14 countries worldwide.  This synthetic dataset is scaled down 1/4 of the original dataset.

Here are the variables.

- step: 1 step is 1 hour. Total steps 744 (30 days simulation).
- type: CASH-IN, CASH-OUT, DEBIT, PAYMENT, and TRANSFER.
- amount: the amount of the transaction in local currency.
- nameOrig: the customer who started the transaction
- oldbalanceOrg: initial balance before the transaction
- newbalanceOrig: new balance after the transaction
- nameDest: the customer who is the recipient of the transaction
- oldbalanceDest: initial balance recipient before the transaction. Note that there is no information for customers that start with M (Merchants).
- newbalanceDest: new balance recipient after the transaction. Note that there is no information for customers that start with M (Merchants).
- isFraud: This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control of customers accounts and try to empty the funds by transferring them to another account and then cashing out of the system.
- isFlaggedFraud: The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is to transfer more than 200.000 in a single transaction.

The goal of this exercise is to find ways to identify if the transaction is fraudulent.  However, pretend that you are not given `isFraud` label since, in many cases, you don't know at the time of the transaction.  You can assume you have access to `isFlaggedFraud` label at the time of your project.  Try to use the method discussed in the chapter to identify fraudulent activities.

```{r}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

### Pokemon Types

Pokemon is a popular game that's been around for ages.  This data set from [Kaggle](https://www.kaggle.com/datasets/abcsds/pokemon) includes 800 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. 

(ID)#: ID for each pokemon
Name: Name of each pokemon
Type 1: Each pokemon has a type, this determines weakness/resistance to attacks
Type 2: Some pokemon are dual type and have 2
Total: sum of all stats that come after this, a general guide to how strong a pokemon is
HP: hit points, or health, defines how much damage a pokemon can withstand before fainting
Attack: the base modifier for normal attacks (eg. Scratch, Punch)
Defense: the base damage resistance against normal attacks
SP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)
SP Def: the base damage resistance against special attacks
Speed: determines which pokemon attacks first each round

```{r}
Pokemon<-fread("Pokemon.csv")
```

Do EDA and find patterns in the data. Notice that this data is multivariate, with some clear dependency in the variables. Does the result make sense?  

```{r}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

Your task is to group the Pokemon into some meaningful clusters that makes sense to you and have a discussion with your neighbor on why your clusters make the most sense. How many cluster seems appropriate?

```{r}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

Look at your cluster and see if you can identify a monster that does not seem to fit with the other monsters. Why do you think that might be?  If you want to know more about these Pokemons you can do a deeper dive by looking through the [database](https://pokemondb.net/pokedex/all).

```{r}
#
#
```

Comment of the result:

~~~
Please write your answer in full sentences.


~~~

### Image cluster

Let's look at Fashion MNIST data.
https://www.kaggle.com/datasets/zalando-research/fashionmnist

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), representing the clothing article. The rest of the columns contain the pixel-values of the associated image.

```{r}
fm<-fread("fashion-mnist_train.csv")
```

- Each row in the data is a separate image.
- Remaining columns are pixel numbers (784 total).  To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. For example, pixel31 indicates the pixel in the fourth column from the left, and the second row from the top.
- Each value is the darkness of the pixel (1 to 255)
- Column 1 is the class label ( which we will assume is not given )
  0 T-shirt/top
  1 Trouser
  2 Pullover
  3 Dress
  4 Coat
  5 Sandal
  6 Shirt
  7 Sneaker
  8 Bag
  9 Ankle boot
  

```{r}
plot_fmnist <- function(imagerow){
gmat<-expand.grid(1:28,1:28)
ggplot(data.frame(x=gmat[,1],y=gmat[,2],value=unlist(imagerow))) + 
  geom_raster(aes(x = x, y = y, fill = value)) + 
  scale_fill_gradient(low = "white", high = "black", na.value = NA) + 
  theme(aspect.ratio = 1, legend.position = "none") + 
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(breaks = seq(0, 28, 7), expand = c(0, 0)) + 
  scale_y_reverse(breaks = seq(0, 28, 7), expand = c(0, 0))
}
plot_fmnist(fm[2,-1])
plot_fmnist(fm[3,-1])
```

## Problem Set


### Euclidian distance and Correlation

In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let $r_{ij}$ denote the correlation between the ith and jth observations, then the quantity $1-r_{ij}$ is proportional to the squared Euclidean distance between the ith and jth observations. On the `USArrests` data, show that this proportionality holds. Hint: The Euclidean distance can be calculated using the `dist()` function, and correlations can be calculated using the `cor()` function.

### Percent Variance Explained

A formula for calculating PVE was given as
$$\frac{\sum_i^n z_{im}^2}{\sum_j^p\sum_i^n x_{ij}^2}=\frac{\sum_i^n (\sum_j^p\phi_{jm}x_{ij}    )^2}{\sum_j^p\sum_i^n x_{ij}^2}$$
We also saw that the PVE can be obtained using the `sdev` output of the `prcomp()` function.
On the `USArrests` data, calculate PVE in two ways:

(a) Using the `sdev` output of the `prcomp()` function, as was done in Section 12.2.3.

(b) By applying Equation 12.10 directly. That is, use the prcomp() function to compute the principal component loadings. Then,
use those loadings in Equation 12.10 to obtain the PVE. These two approaches should give the same results.
Hint: You will only obtain the same results in (a) and (b) if the same
data is used in both cases. For instance, if in (a) you performed prcomp() using centered and scaled variables, then you must center
and scale the variables before applying Equation 12.10 in (b).

### Hierarchical clustering of USA Arrest

Consider the `USArrests` data. We will now perform hierarchical clustering on the states.
(a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Simulation PCA and Kmeans

In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

(a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.
Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; `runif()` is
another option. Be sure to add a mean shift to the observationsin each class so that there are three distinct classes.
Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(b) Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate
the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If
not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not
continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(c) Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering
compare to the true class labels? 
Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful
how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true
class labels and clustering labels are the same.
Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(d) Perform K-means clustering with K = 2. Describe your results.
Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(e) Now perform K-means clustering with K = 4, and describe your results.
Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(f) Now perform K-means clustering with K = 3 on the first two
principal component score vectors, rather than on the raw data.
That is, perform K-means clustering on the 60 × 2 matrix of
which the first column is the first principal component score
vector, and the second column is the second principal component
score vector. Comment on the results.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

(g) Using the `scale()` function, perform K-means clustering with
K = 3 on the data after scaling each variable to have standard
deviation one. How do these results compare to those obtained
in (b)? Explain.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Matrix completion

Write an R function to perform matrix completion as in Algorithm 12.1, and as outlined in Section 12.5.2. In each iteration, the function should keep track of the relative error, as well as the iteration count. Iterations should continue until the relative error is small enough or until some maximum number of iterations is reached (set a default value for this maximum number). Furthermore, there should be an option to print out the progress in each iteration. Test your function on the Boston data. First, standardize the features to have mean zero and standard deviation one using the scale() function. Run an experiment where you randomly leave out an increasing (and nested) number of observations from 5% to 30%, in steps of 5%. Apply Algorithm 12.1 with M = 1, 2, . . . , 8. Display the approximation error as a function of the fraction of observations that are missing, and the value of M, averaged over 10 repetitions of the experiment.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

### Matrix completion


In Section 12.5.2, Algorithm 12.1 was implemented using the svd() function. However, given the connection between the svd() function and the prcomp() function highlighted in the lab, we could have instead implemented the algorithm using prcomp().
Write a function to implement Algorithm 12.1 that makes use of prcomp() rather than svd().

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


### Gene expression

The dataset on gene expression (Ch12Ex13.csv) consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.
```{r}
credit_data <- read.csv("Ch12Ex13.csv",header=FALSE )
```

(a) Load in the data using `read.csv()`. You will need to select `header = F`.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(b) Apply hierarchical clustering to the samples using correlationbased distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(c) Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~




## Additional Material


### pheatmap

pheatmap is popular in bioinformatics because it combines hierarchical clustering, heatmap, and k-means at once.  You can row cluster and column cluster at the same time.  

```{r,fig.width=7,fig.height=7.5}
#source("https://bioconductor.org/biocLite.R")
#biocLite("pheatmap")
library(pheatmap)
iris_dt<-iris
rownames(iris_dt)<-(paste0("sample",1:150))
annotr<- data.frame(iris_dt$Species)
rownames(annotr) <-paste0("sample",1:150)
pheatmap(iris_dt[,-5],clustering_method = "ward.D", clustering_distance_rows="minkowski",annotation_row =annotr )
```

### visualizing K-means result

There is nice visualization for k-means clustering.

```{r,fig.width=7,fig.height=7.5}
library(cluster)
set.seed(2)
x=matrix(rnorm(50*2), ncol=2) #2 dimensional
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
km.out=kmeans(x,3,nstart=20)
clusplot(x, km.out$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)
```

```{r,fig.width=7,fig.height=7.5}
# Centroid Plot against 1st 2 discriminant functions
library(fpc)
plotcluster(x, km.out$cluster) 
```

Another nice way to visualize K-means results when there are not too many k is the radar chart.
Returning to the USArrest example, we can use k-means and then inspect whether the clusters make sense.

```{r,fig.width=7,fig.height=7.5}
km.usA=kmeans(USArrests,5,nstart=1)
df<-as.data.frame(scale(km.usA$centers))
dfmx<-apply(df,2,max)+1
dfmn<-apply(df,2,min)-1
df<-rbind(dfmx,dfmn,df)
fmsb::radarchart(df,seg=5,plty=1,pcol=brewer.pal(5,"Set1"))
legend("topright",legend=1:5,col=brewer.pal(5,"Set1"),lty=1)

states <- map_data("state")

dfft<-data.frame(region=(tolower(names(km.usA$cluster))),cluster=factor(km.usA$cluster))
library(dplyr)
statec <- left_join(states, dfft, by = "region")
ggplot(data = statec) + 
  geom_polygon(aes(x = long, y = lat, fill = cluster, group = group), color = "white") + 
  coord_fixed(1.3) +scale_fill_brewer(palette="Set1")
```

### Choice of the distance and linkage

Weird things happen for a particular choice of linkage methods.

This is an example where using the centroid flips.

```{r ,fig.width=8,fig.height=3}
x<-c(5.24,4.58,6.25,6.15,5.02,5.95)
y<-c(3.16,2.78,2.24,4.04,3.99,2.77)
df<-data.frame(x,y)
par(mfrow=c(1,3))
plot(x,y,type="n"); text(x,y,1:6)
plot(hclust(dist(df),method = "single"))
plot(hclust(dist(df),method = "centroid"))
```

The different combinations will return different results.  Knowing that you cannot be ignorant about the method is essential.  Below I used the Iris data and utilized all the distance and linkage methods combinations.  The point is not to do such a thing but to let you know that some combinations are wrong and others may or may not return sensible results.  

```{r}
link_Methods <-c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median" , "centroid")
dist_Methods <-c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
```

```{r}
hclustlist<- vector("list",length(link_Methods))
for( i in 1:length(link_Methods)) hclustlist[[i]]<- vector("list",length(dist_Methods))

classlist<- vector("list",length(link_Methods))
for( i in 1:length(link_Methods)) classlist[[i]]<- vector("list",length(dist_Methods))

```

```{r,fig.width=10,fig.height=20,fig.align = "center"}
for( i in 1:length(link_Methods)) {
  for( j in 1:length(dist_Methods)){
    hclustlist[[i]][[j]] <-hclust(stats::dist(iris[,-5],dist_Methods[j]),method=link_Methods[i])
    classlist[[i]][[j]] <-cutree(hclustlist[[i]][[j]],k=3)
  }
}
library(abind)
#res<-cbind(iris$Species,abind(lapply(classlist,abind,along=2),along=2))
res<-abind(lapply(classlist,abind,along=2),along=2)
dimnames(res)<-list(paste0("sample",1:150),paste0("exp",1:48))
library(pheatmap)
annotr<-data.frame(ans=iris$Species)
annotc<-(data.frame(link=factor(rep(link_Methods,each=length(dist_Methods))),
                    distance=factor(rep(dist_Methods,length(link_Methods)))))
rownames(annotr) <-paste0("sample",1:150)
rownames(annotc)<-paste0("exp",1:48)
pheatmap(res,annotation_row = annotr,annotation_col = (annotc),cluster_rows = FALSE,clustering_distance_cols = "manhattan")
```

```{r,fig.width=20,fig.height=20, out.width="1\\linewidth",fig.align = "center"}
par(mfrow=c(length(dist_Methods),length(link_Methods)))
for( i in 1:length(link_Methods)) {
  for( j in 1:length(dist_Methods)){
  plot(hclustlist[[i]][[j]])
  }
}

```

### Other distances

There are many different distances and some are good for binary variables.

- euclidean	$$d_{jk} = \sqrt{(\sum(x_{ij}-x_{ik})^2)}$$
	binary:$sqrt(A+B-2*J)$
- manhattan	$$d_{jk} = \sum(|x_{ij} - x_{ik}|)$$
	binary:$A+B-2*J$
- gower	$$d_{jk}  = \frac{1}{M} \sum\left(\frac{|x_{ij}-x_{ik}|}{(\max(x_{i})-\min(x_{i}))}\right)$$
	binary:$(A+B-2*J)/M$,
	where M is the number of columns (excluding missing values)
- altGower	$$d_{jk}  = ( \frac{1}{NZ} ) \sum(|x_{ij} - x_{ik}|)$$
	where $NZ$ is the number of non-zero columns excluding double-zeros (Anderson et al. 2006).
	binary:$(A+B-2*J)/(A+B-J)$
- canberra	$$d_{jk} = \frac{1}{NZ}  \sum \left(\frac{(x_{ij}-x_{ik})}{(x_{ij}+x_{ik})}\right)$$
	whereNZis the number of non-zero entries.
	binary:$(A+B-2*J)/(A+B-J)$
- bray	$$d_{jk}  = \left(\sum |x_{ij}-x_{ik}| \right)/\left(\sum (x_{ij}+x_{ik})\right)$$
	binary:$(A+B-2*J)/(A+B)$
- kulczynski	$$d_{jk} 1 - 0.5*((\sum \min(x_{ij},x_{ik})/(\sum x_{ij}) + (\sum min(x_{ij},x_{ik})/(\sum x_{ik}))$$
	binary:$1-(J/A + J/B)/2$
- morisita	$$d_{jk} = 1 - 2*\frac{\sum\left(x_{ij}*x_{ik}\right)}{((\lambda_{j}+\lambda_{k}) * \sum(x_{ij})*\sum(x_{ik}))}$$, where
	$lambda_{j} = \sum(x_{ij}*(x_{ij}-1))/\sum(x_{ij})*\sum(x_{ij}-1)$
	binary: cannot be calculated
- horn	Like morisita, but $$lambda_{j} = \sum(x_{ij}^2)/(\sum(x[ij])^2)$$
	binary:$(A+B-2*J)/(A+B)$
- binomial	$$d_{jk} = \sum\left(x_{ij}*\log(\frac{x_{ij}}{n_{i}}) + x_{ik}*\log(\frac{x_{ik}}{n_{i}}) - n_{i}*\log( \frac{1}{2} )\right)/n_{i}$$,
	where $n_{i} = x_{ij} + x_{ik}$
	binary:$\log(2)*(A+B-2*J)$
- cao	$$d_{jk}  = \frac{1}{S} * sum(log(n_{i}/2) - (x_{ij}*\log(x_{ik}) + x_{ik}*\log(x_{ij}))/n_{i})$$,
	where $S$ is the number of species in compared sites and $n_{i} = x_{ij} + x_{ik}$

You can find these in the vegan package.
```{r}
vdist_Methods<-c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "morisita", "horn", "mountford", "raup" , "binomial", "chao", "cao","mahalanobis")
vegan::vegdist
xx<-t(matrix(rbinom(1000,1,0.5),100))
dist(xx)
vegan::vegdist(xx)
```

### Cluster performance indices

There are various indices proposed for comparing the clustering result. `NbClust` can calculate many of these statistics for you.  Here is synthetic data with 6 clusters.

```{r,fig.width=7,fig.height=7.5}
n = 100
g = 6 
set.seed(7)
d <- data.frame(x = unlist(lapply(1:g, function(i) rnorm(n/g, runif(1)*i^2))), 
                y = unlist(lapply(1:g, function(i) rnorm(n/g, runif(1)*i^2))))
plot(d)
```
If we manually calculate the WGSS
```{r}
mydata <- d
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata,
                                       centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

```
Using NbClust
```{r}
nb <- NbClust(d, distance = "euclidean", 
        min.nc=2, max.nc=15, method = "kmeans", 
        index = "alllong", alphaBeale = 0.1)
hist(nb$Best.nc[1,], breaks = max(na.omit(nb$Best.nc[1,])))
```

### Partitioning Around Medoids
Partitioning (clustering) of the data into k clusters “around medoids” is a more robust version of K-means.
```{r}
library(cluster)
iris_pam<-pam(iris[,-5], 3, diss = FALSE)
iris_pam<-pam(iris[,-5], 3, diss = FALSE)
iris_kms<-kmeans(iris[,-5], 3)
#iris_pam$clustering
#iris_kms$cluster
table(iris$Species,iris_kms$cluster)
table(iris$Species,iris_pam$clustering)
```
### Mclust

Mclust is a package with a collection of model based clustering methods.
```{r}
library(mclust)
mc <- Mclust(iris[,1:4], 3)
summary(mc)
par(mfrow=c(1,2))
plot(mc, what=c("classification"), dimens=c(1,2))
plot(mc, what=c("classification"), dimens=c(3,4))
```

### Unsupervised Learning for Timeseries

Working with timeseries data in Unsupervised Learning situations poses several unique challenges. 
The naive way of treating observations along time as a set of features works well only when the timeseries' are well aligned.
For many realistic situations, this is not necessarily the case. Even in a situation where the comparison is about the aligned timeseries, there are problems such as missing data and misalignment of the starting times that need treatment.

To make the point, here is CO2 emission by country.  
```{r,echo=FALSE}
#https://github.com/opencasestudies/ocs-bp-co2-emissions/tree/master/data/raw
CO2_emissions <- readxl::read_xlsx("yearly_co2_emissions_1000_tonnes.xlsx")

memm<-melt(CO2_emissions[which(CO2_emissions$country%in%c("United States","Canada","China","United Kingdom","France","India")),],id.vars = "country")

ggplot(memm)+geom_point()+geom_line()+
  aes(x=as.integer(as.character(variable)),y=value,color=country)+
  scale_y_log10()+xlab("year")+ylab("Co2 Emission") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
```
Let's say you want to cluster the countries by their trend.  The time is aligned since each country has the same years.
However, you can see that the measurement goes back much longer in time for UK than other countries. Also there are gaps in the measurements that need to be taken care of.  One way to deal with such data is to limit your observations to years that are more stable.

```{r}
ggplot(memm[as.integer(as.character(memm$variable))>2000,])+
  geom_point()+geom_line()+
  aes(x=as.integer(as.character(variable)),y=value,color=country)+
  scale_y_log10()+xlab("year")+ylab("Co2 Emission")
```

Another example is to determine the letter being written from the trajectories of a writer. Below are lines for five examples of the X velocity for A and B.
```{r}
par(mfrow=c(1,2))
plot(CharTraj[[1]],type="l",main="A",ylim=c(-1,1))
lines(CharTraj[[2]])
lines(CharTraj[[3]])
lines(CharTraj[[4]])
lines(CharTraj[[5]])

plot(CharTraj[[6]],type="l",main="B",ylim=c(-1,1))
lines(CharTraj[[7]])
lines(CharTraj[[8]])
lines(CharTraj[[9]])
lines(CharTraj[[10]])
```
This will be a whole timeseries comparison within fixed time window, but you have a misalignment in time that would distort point-by-point distance.

Suppose the goal is to extract particular activity from measurements taken on a wearable device and to classify what is being done. In that case, an additional segmentation issue must be considered.

#### Dynamic Time Warping (DTW)

Dynamic Time warping is a way to calculate distances between datapoints when the datapoints are shifted horizontally in time between each other, but the shape is consistent.  Unlike Euclidean distance, for DTW, the two time series do not have to have an equal length. DTW calculates the smallest distance between all points and then finds the closest matching points.  Here is an illustration of how it works. (https://rpubs.com/esobolewska/dtw-time-series)

```{r,echo=FALSE}
a1 <- c(7,9,6,9,12,6,4,6,8)
a2 <- c(5,6,4,3,9,5,6,8,9)
dtwfit<-dtw(a1,a2,keep=TRUE)
xrange <- range(1:9)
yrange <- range(c(a1,a2))
plot(xrange, yrange, type="n", xlab="time",
     ylab="value", xaxp  = c(0,10,10), yaxp  = c(3,12,9)) 
lines(a1, col='blue', type='l')
lines(a2, col='magenta', type='l')
```

```{r}
plot(dtwfit, xlab="a1 - blue", ylab="a2 - magenta", 
     xaxp  = c(0,10,10), yaxp = c(0,10,10), type="threeway")
plot(dtwfit, xaxp  = c(0,10,10), yaxp = c(0,10,10), type="twoway", col=c('blue', 'magenta'))
```

```{r}
co2emissions.norm <- BBmisc::normalize(CO2_emissions[complete.cases(CO2_emissions[,255:265]),255:265], method="standardize")
clust.pam <- tsclust(co2emissions.norm, type="partitional", k=6L, distance="dtw", centroid="pam")
plot(clust.pam, type = "sc")
```

https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf

Hierarchical clustering of handwriting data would look like this:
```{r}
hc <- tsclust(CharTraj, type = "hierarchical", k = 20L, 
              distance = "sbd", trace = TRUE,
              control = hierarchical_control(method = "average"))
plot(hc)


tdf<-melt(CharTraj) # %>% group_by(L1) %>% mutate(id = row_number())
tcf<-melt(hc@cluster)
tcf$L1<-rownames(tcf)
tdf<-merge(tdf,tcf,by ="L1")
tdf$num <- sequence(rle(tdf$L1)$lengths)
ggplot(tdf)+geom_line(aes(x=num,y=value.x,group=L1))+facet_wrap(~value.y)
```



### DBScan

The DBScan clustering algorithm is as follows:

1. Randomly select a point i.
2. Retrieve all the points that are density reachable from i concerning the Maximum radius of the neighborhood (EPS) and the minimum number of points within eps neighborhood(Min Pts).
3. If the number of points in the neighborhood is more than Min Pts then i is a core point.
4. For i core points, a cluster is formed. If i is not a core point, mark it as a noise/outlier and move to the next point.
5. Continue the process until all the points have been processed.

```{r,fig.width=7,fig.height=7.5}
library(fpc)

set.seed(12345)  # Setting seed
Dbscan_cl <- dbscan(iris[,1:4], eps = 0.45, MinPts = 5)
Dbscan_cl
  
# Checking cluster
Dbscan_cl$cluster
  
# Table
table(Dbscan_cl$cluster, iris$Species)
  
# Plotting Cluster
plot(Dbscan_cl, iris[,1:4], main = "DBScan")
plot(Dbscan_cl, iris[,1:4], main = "Petal Width vs Sepal Length")
```


### Other dimension reduction techniques

Various other dimension reduction techniques have been popularized over the years.


####  t-distributed stochastic neighbor embedding (t-SNE) 

First is [t-SNE](https://lvdmaaten.github.io/tsne/) by L.J.P. van der Maaten and G.E. Hinton (2008). The algorithm projects the high-dimensional data points into low dimension (2D) by inducing the projected data to have a similar distribution as the original data points by minimizing KL divergence.

```{r,eval=TRUE}
library(Rtsne)
# perform dimensionality reduction from 64D to 2D
tsne_out <- Rtsne(as.matrix(fm[1:200,-1]), check_duplicates = FALSE, 
              pca = FALSE, perplexity=20, theta=0.5, dims=2)

cols <- rainbow(10)

tsne_plot <- data.frame(x = tsne_out$Y[,1],
                        y = tsne_out$Y[,2],label=factor(unlist(fm[1:200,1])))
 
# Plotting the plot using ggplot() function
ggplot2::ggplot(tsne_plot)+ geom_point(aes(x=x,y=y,color=label)) 

#plot(tsne_out$Y[,1],tsne_out$Y[,2],col=cols[unlist(fm[1:200,1]) +1] )
#text(tsne$Y[,1],tsne$Y[,2], labels=fm[1:200,1], col=cols[unlist(fm[1:200,1]) +1])
```

#### Uniform Manifold Approximation (UMAP)

UMAP is another dimension reduction described by McInnes and Healy (2018) in <arXiv:1802.03426>.

```{r}
library(umap)
custom.config <- umap.defaults
config=custom.config
custom.config$n_epochs <-500
umapresult<-umap(fm[1:200,-1],config=custom.config)
```

```{r}
plot_umap<-function(x, labels,
         main="A UMAP visualization",
         colors=rainbow(10),
         pad=0.1, cex=0.6, pch=19, add=FALSE, legend.suffix="",
         cex.main=1, cex.legend=0.85) {

  layout <- x
  if (is(x, "umap")) {
    layout <- x$layout
  } 
  
  xylim <- range(layout)
  xylim <- xylim + ((xylim[2]-xylim[1])*pad)*c(-0.5, 0.5)
  if (!add) {
    par(mar=c(0.2,0.7,1.2,0.7), ps=10)
    plot(xylim, xylim, type="n", axes=F, frame=F)
    rect(xylim[1], xylim[1], xylim[2], xylim[2], border="#aaaaaa", lwd=0.25)  
  }
  points(layout[,1], layout[,2], col=colors[as.integer(labels)],
         cex=cex, pch=pch)
  mtext(side=3, main, cex=cex.main)

  labels.u <- unique(labels)
  legend.pos <- "topleft"
  legend.text <- as.character(labels.u)
  if (add) {
    legend.pos <- "bottomleft"
    legend.text <- paste(as.character(labels.u), legend.suffix)
  }

  legend(legend.pos, legend=legend.text, inset=0.03,
         col=colors[as.integer(labels.u)],
         bty="n", pch=pch, cex=cex.legend)
}
plot_umap(umapresult,unlist(fm[1:200,1]))

```


### Using H2O for Unsupervised Learning

```{r}
library(h2o)
localH2O<-h2o.init()
```

```{r}
ausPath   = system.file("extdata", "australia.csv", package = "h2o")

print(paste("Uploading", ausPath))
australia.hex <- h2o.uploadFile(path = ausPath,
                                destination_frame = "australia.hex")

print("Print out summary of australia.csv")
print(summary(australia.hex))

```

#### PCA

H2O uses the power method to calculate the singular value decomposition of the Gram matrix.

```{r}
print("Run PCA with k = 8, gamma = 0, transform = 'DEMEAN'")
australia.pca = h2o.prcomp(australia.hex, k = 8, transform = "DEMEAN")
print(australia.pca)

print("Run PCA with k = 4, transform = 'STANDARDIZE'")
australia.pca2 = h2o.prcomp(australia.hex, k = 4, transform = "STANDARDIZE")
print(australia.pca2)

h2o.screeplot(australia.pca2, type = c("barplot"))
australia.pca2@model$eigenvectors
```

#### K-means

```{r}
prosPath = system.file("extdata", "prostate.csv", package="h2o")
print(paste("Uploading", prosPath))
prostate.hex =  h2o.uploadFile(path = prosPath)

print("Print out summary of prostate.csv")
print(summary(prostate.hex))

print("Run K-means with k = 10")
prostate.kmeans<-h2o.kmeans(training_frame = prostate.hex, k = 10, 
           x = c("AGE", "RACE", "VOL", "GLEASON"))
print(prostate.kmeans)
h2o.cluster_sizes(prostate.kmeans, train = TRUE)
h2o.centers(prostate.kmeans)
```

#### Shut down H2O
```{r}
h2o.shutdown(prompt =F)
sessionInfo()
```
